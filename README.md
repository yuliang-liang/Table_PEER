# PEER


## Abstract
Recent advances in Large Vision-Language Models (LVLMs) have sparked growing interest in multimodal table understanding, as table images are widely available in the real world. However, existing approaches largely rely on costly instruction tuning, which limits scalability across diverse LVLMs and practical deployment, especially with black-box APIs such as GPT-4. 
To address these challenges, we propose a training-free, self-improving prompting framework to enhance LVLMs’ table understanding ability. Inspired by Dual-system theory, we devise three key inference stages—**Perception**, **Reasoning**, and **Reflection**—that progressively seek evidence in table images and iteratively refine derived answers. This staged process emulates deliberate System 2 reasoning, thereby reducing issues such as information omission and hallucination. 





## Datasets
Refer to the multimodal table understanding dataset in the [MMTab Dataset](https://huggingface.co/datasets/SpursgoZmy/MMTab).

*Note: We use the queries from the original dataset (e.g., WTQ and TabFact) and table images generated by MMtab.*


## Code

- PEER
```
python run_peer.py
```
- Base & CoT
```
python run_naive.py
```
## Arguments
We set the arguments in the source code.
- `--model_name`: name of the huggingface LVLMs: `Qwen2-VL-7B-Instruct` / `Llama-3.2-11B-Vision-Instruct`
- `--task_type`: task type: `TQA` / `TFV`
- `--dataset_name`:  for TQA : `TABMWP / WTQ  / HiTab / TAT-QA` , for TFC： `TabFact / InfoTabs `
- `--method`: the prompt method: `PEER / naive / cot` 




## Acknowledgement
Our implementation references the code below, thanks to them.

- Chain of Table : https://github.com/google-research/chain-of-table

- Multimodal-Table-Understanding: https://github.com/SpursGoZmy/Table-LLaVA



<!-- [Task Arithmetic](https://github.com/mlfoundations/task_vectors), [TIES-MERGING](https://github.com/prateeky2806/ties-merging/tree/main), [Model Soups](https://github.com/mlfoundations/model-soups) -->
