# PEER


## Abstract
Table understanding aims to extract structured knowledge and answer table-related queries. Large Vision-Language Models (LVLMs), with their cross-modal capabilities, offer a promising avenue for directly understanding tables by leveraging visual information. However, their pre-training primarily targets general vision-language tasks, leading to underperformance in specialized table-based tasks. Existing solutions rely heavily on fine-tuning with large-scale labeled tabular data, leading to high computational costs and struggles to generalize across diverse table formats and unseen domains. This highlights the urgent need for more efficient and generalizable approaches to table understanding. In this paper, we explore and propose a training-free self-improving paradigm to enhance LVLMs' table understanding. Inspired by human cognition, our approach includes three key stages: *Perception*, *Reasoning*, and *Reflection*. Specifically, in the Perception stage, LVLMs extract query-relevant textual evidence from table images. The Reasoning stage summarizes this evidence to produce an initial answer. The Reflection stage revisits the query and initial answer to identify missing or supplementary evidence for further reasoning the answer. By alternating between Reflection and Reasoning, the model autonomously mitigates issues like information omissions and ensures robust responses. Extensive experiments on six table benchmarks validate the effectiveness, achieving averaging 14% relative improvement on Qwen2-VL-7B and 28% on Llama-3.2-11B-Vision.



## Datasets
Refer to the multimodal table understanding dataset in the [MMTab Dataset](https://huggingface.co/datasets/SpursgoZmy/MMTab).

*Note: We use the queries from the original dataset (e.g., WTQ and TabFact) and table images generated by MMtab.*


## Code

- PEER
```
python run_autotab.py
```
- Base & CoT
```
python run_naive.py
```
## Arguments
We set the arguments in the source code.
- `--model_name`: name of the huggingface LVLMs: `Qwen2-VL-7B-Instruct` / `Llama-3.2-11B-Vision-Instruct`
- `--task_type`: task type: `TQA` / `TFV`
- `--dataset_name`:  for TQA : `TABMWP / WTQ  / HiTab / TAT-QA` , for TFCï¼š `TabFact / InfoTabs `
- `--method`: the prompt method: `PEER / naive / cot` 




## Acknowledgement
Our implementation references the code below, thanks to them.

- Chain of Table : https://github.com/google-research/chain-of-table

- Multimodal-Table-Understanding: https://github.com/SpursGoZmy/Table-LLaVA



<!-- [Task Arithmetic](https://github.com/mlfoundations/task_vectors), [TIES-MERGING](https://github.com/prateeky2806/ties-merging/tree/main), [Model Soups](https://github.com/mlfoundations/model-soups) -->
