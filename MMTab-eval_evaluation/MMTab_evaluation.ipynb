{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import re\n",
    "import tqdm\n",
    "from collections import defaultdict\n",
    "from sacrebleu.metrics import BLEU\n",
    "from metric import TEDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_table_to_html_str(table_row_list=[]):\n",
    "    \"\"\"\n",
    "    Given a list of table rows, build the corresponding html string, which is used to compute the TEDS score.\n",
    "    We use the official code of PubTabNet to compute TEDS score, it does not consider '<th>' label.\n",
    "    We also remove unneccessary spaces within a table cell and extra '\\n' as they will influence the TEDS score.\n",
    "    \"\"\"\n",
    "    html_table_str = \"<html><body><table>\" + '\\n'\n",
    "    for data_row in table_row_list:\n",
    "        html_table_str += \"<tr>\"\n",
    "        for cell_str in data_row:\n",
    "            html_table_str += f\"<td>{cell_str}</td>\"\n",
    "        html_table_str += \"</tr>\"\n",
    "        html_table_str += '\\n'\n",
    "    html_table_str += \"</table></body></html>\"\n",
    "    html_table_str = html_table_str.replace('\\n','')\n",
    "    return html_table_str\n",
    "\n",
    "def convert_markdown_table_to_html(markdown_table):\n",
    "    \"\"\"\n",
    "    Converts a markdown table to the corresponding html string for TEDS computation.\n",
    "    \"\"\"\n",
    "    # remove extra code block tokens like '```markdown' and '```\n",
    "    markdown_table = markdown_table.strip('```markdown').strip('```').strip() \n",
    "    row_str_list = markdown_table.split('\\n')\n",
    "    # extra the first header row and other data rows\n",
    "    valid_row_str_list = [row_str_list[0]]+row_str_list[2:]\n",
    "    table_rows = []\n",
    "    for row_str in valid_row_str_list:\n",
    "        one_row = []\n",
    "        for cell in row_str.strip().split('|')[1:-1]:\n",
    "            if set(cell) != set(' '):\n",
    "                one_row.append(cell.strip())\n",
    "            else:\n",
    "                one_row.append(' ')\n",
    "        table_rows.append(one_row)\n",
    "    # build html string based on table rows\n",
    "    html_str = convert_table_to_html_str(table_rows)\n",
    "    return html_str\n",
    "\n",
    "def convert_latex_table_to_html(latex_table):\n",
    "    \"\"\"\n",
    "    Converts a markdown table to html string for TEDS computation.\n",
    "    In the MMTab-eval, we only consider latex tables with similar structures of markdown tables.\n",
    "    For other latex tables with compicated structures like merged cells, you need to rewrite this function to convert them.\n",
    "    \"\"\"\n",
    "    # remove extra code block tokens like '```latex' and '```\n",
    "    latex_table = latex_table.strip('```latex').strip('```').strip() \n",
    "    latex_table = latex_table.replace('\\n', ' ')\n",
    "    row_str_list = [row_str.strip('\\n').strip('\\\\') for row_str in latex_table.split('\\hline')[1:-1]]\n",
    "    table_rows = []\n",
    "    for row_str in row_str_list:\n",
    "        one_row = []\n",
    "        for c in row_str.split('&'):\n",
    "            if set(c) != set(' '):\n",
    "                one_row.append(c.strip())\n",
    "            else:\n",
    "                one_row.append(' ')\n",
    "        table_rows.append(one_row)\n",
    "    html_str = convert_table_to_html_str(table_rows)\n",
    "    return html_str\n",
    "\n",
    "def wrap_html_table(html_table):\n",
    "    \"\"\"\n",
    "    The TEDS computation from PubTabNet code requires that the input html table should have <html>, <body>, and <table> tags.\n",
    "    Add them if they are missing.\n",
    "    \"\"\"\n",
    "    html_table = html_table.replace('\\n','')\n",
    "    # add missing <table> tag if missing\n",
    "    if \"<table\" in html_table and \"</table>\" not in html_table:\n",
    "        html_table = html_table + \"</table>\"\n",
    "    elif \"<table\" not in html_table and \"</table>\" in html_table:\n",
    "        html_table = \"<table>\" + html_table\n",
    "    elif \"<table\" not in html_table and \"</table>\" not in html_table:\n",
    "        html_table = \"<table>\" + html_table + \"</table>\"\n",
    "    else:\n",
    "        pass\n",
    "    # add <body> and <html> tags if missing\n",
    "    if '<body>' not in html_table:\n",
    "        html_table = '<body>' + html_table + '</body>'\n",
    "    if '<html>' not in html_table:\n",
    "        html_table = '<html>' + html_table + '</html>'\n",
    "    return html_table\n",
    "\n",
    "# Read inference results of LLaVA model (merged.jsonl)\n",
    "def read_llava_prediction_file(file_path):\n",
    "    \"\"\"\n",
    "    Read LLaVA's inference results (e.g., merge.jsonl) and extract data of different benchmarks based on 'category' field.\n",
    "    \"\"\"\n",
    "    predict_results = []\n",
    "    with open(file_path, encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in tqdm.tqdm(lines):\n",
    "            item = json.loads(line.strip())\n",
    "            predict_results.append(item)\n",
    "    print(\"Predicted Sample Number:\",len(predict_results))\n",
    "    benchmark_name_to_predicted_item_list = defaultdict(list)\n",
    "    for item in predict_results:\n",
    "        item_id = item['question_id']\n",
    "        category = item['category'] # {dataset_name}_for_{task_name}, e.g., TabFact_for_TFV\n",
    "        dataset_name = category.split('_for_')[0] # e.g., TabFact\n",
    "        task_name = category.split('_for_')[1] # e.g., TFV\n",
    "        # for table structure understanding tasks, benchmark name is the task name\n",
    "        if task_name not in ['TSD','TCL','RCE','MCD','TCE','TR','OOD_TSD','OOD_TCL','OOD_RCE','OOD_TCE']:\n",
    "            benchmark_name = dataset_name\n",
    "        else:\n",
    "            benchmark_name = task_name\n",
    "        benchmark_name_to_predicted_item_list[benchmark_name].append(item)\n",
    "    for benchmark_name,  predicted_item_list in benchmark_name_to_predicted_item_list.items():\n",
    "        item_num = len(predicted_item_list)\n",
    "        print(f'benchmark name: {benchmark_name}, test data num: {item_num}')\n",
    "    return benchmark_name_to_predicted_item_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Read Predicted Data and Ground Truth Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4344/4344 [00:00<00:00, 129965.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sample Number: 4344\n",
      "benchmark name: WTQ, test data num: 4344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# read the predicted data\n",
    "benchmark_name_to_predicted_item_list = read_llava_prediction_file(\"/gly/guogb/lyl/Visual-Table/answer.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMTab-eval data num:  49726\n",
      "MMTab-eval table num:  23930\n"
     ]
    }
   ],
   "source": [
    "# read the ground truth data\n",
    "MMTab_eval_test_data = json.load(open(\"/gly/guogb/lyl/Datasets/MMTab/MMTab-eval_test_data_49K.json\"))\n",
    "# item_id --> test data\n",
    "item_id_to_test_item = {}\n",
    "for item in MMTab_eval_test_data:\n",
    "    item_id = item['item_id']\n",
    "    item_id_to_test_item[item_id] = item\n",
    "print(\"MMTab-eval data num: \",len(MMTab_eval_test_data))\n",
    "# table_id --> test table\n",
    "MMTab_eval_test_tables = json.load(open(\"/gly/guogb/lyl/Datasets/MMTab/MMTab-eval_test_tables_23K.json\"))\n",
    "table_id_to_test_table = {}\n",
    "for table_item in MMTab_eval_test_tables:\n",
    "    table_id = table_item['image_id']\n",
    "    table_id_to_test_table[table_id] = table_item\n",
    "print(\"MMTab-eval table num: \",len(table_id_to_test_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'item_id': 'TSD_test_item_0',\n",
       " 'image_id': 'TABMWP_21652',\n",
       " 'input': 'I need to know the count of rows and columns in this specific table. Format your final answer as a JSON, using the structure {\"row_number\": \"m\", \"column_number\": \"n\"}.',\n",
       " 'output': 'This table has 3 rows and 3 columns. Thus, the final answer is {\"row_number\": \"3\", \"column_number\": \"3\"}.',\n",
       " 'task_type': 'TSD',\n",
       " 'dataset_name': 'TABMWP',\n",
       " 'original_query': 'None',\n",
       " 'answer_list': [[3, 3]],\n",
       " 'original_query_type': 'table size detection'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MMTab_eval_test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_id': 'TABMWP_8',\n",
       " 'table_rows': [['Cookies baked', 'Cookies baked'],\n",
       "  ['Day', 'Number of cookies'],\n",
       "  ['Friday', '163'],\n",
       "  ['Saturday', '281'],\n",
       "  ['Sunday', '263']],\n",
       " 'table_title': 'Cookies baked',\n",
       " 'dataset_name': 'TABMWP'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_id_to_test_table['TABMWP_8']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Evaluation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 TQA, TFV and T2T Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tqa_answer_list(model_output):\n",
    "    \"\"\"\n",
    "    Extract the answer list from the model output to compute accuracy\n",
    "    \"\"\"\n",
    "    model_output = model_output.replace('\\n',' ')\n",
    "    ret = re.match('.*({[\\\"\\']answer[\\\"\\']\\:.*}).*',model_output)\n",
    "    if ret is not None:\n",
    "        answer_str = ret.group(1)\n",
    "        try:\n",
    "            answer_str = re.sub('[\\\"\\']+',\"\\\"\",answer_str)\n",
    "            answer_item = eval(answer_str)\n",
    "            predicted_answer = answer_item['answer']\n",
    "            if type(predicted_answer) != list and type(predicted_answer) == str:\n",
    "                predicted_answer = [predicted_answer]\n",
    "            elif type(predicted_answer) != list and type(predicted_answer) in [float,int]:\n",
    "                predicted_answer = [str(predicted_answer)]\n",
    "            else:\n",
    "                pass\n",
    "        # The answer is considered to be wrong if we can not extract answer list from the json str\n",
    "        except:\n",
    "            predicted_answer = []\n",
    "        return predicted_answer\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def evaluate_tqa_questions(benchmark_name,pred_item_list):\n",
    "    \"\"\"\n",
    "    Evaluation for table question answering (TQA) and table fact verification (TFV) benchmark.\n",
    "    Metric: accuracy.\n",
    "    Note that some baseline models can not strictly follow instructions to output the final answer in the required JSON format.\n",
    "    For instance, Qwen-VL may only output a short answer due to the potential overfitting of training data.\n",
    "    In such cases, the evaluation script needs to be changed according to the characteristic of certain model output.\n",
    "    \"\"\"\n",
    "    correct_item_list = []\n",
    "    wrong_item_list = []\n",
    "    failed_item_list = []\n",
    "    for item in pred_item_list:\n",
    "        try:\n",
    "            item_id = item['question_id']\n",
    "            ori_item = item_id_to_test_item[item_id]\n",
    "            model_output = item['text']\n",
    "            # parse the predicted answer list\n",
    "            predicted_answer_list = extract_tqa_answer_list(model_output)\n",
    "            gold_answer_list = ori_item['answer_list']\n",
    "            # Sometimes the order of multiple answer text is not necessarily same as the gold answer,\n",
    "            # so we convert the answer list to a set for comparison\n",
    "            if set(gold_answer_list) == set(predicted_answer_list):\n",
    "                correct_item_list.append(item)\n",
    "            else:\n",
    "                wrong_item_list.append(item)\n",
    "        except Exception:\n",
    "            failed_item_list.append(item)\n",
    "            \n",
    "    print(\"Benchmark: \",benchmark_name)\n",
    "    correct_num = len(correct_item_list)\n",
    "    total_sample_num = len(pred_item_list)\n",
    "    print(\"Accuracy: \", correct_num/total_sample_num)\n",
    "    problem_sample_num = len(failed_item_list)\n",
    "    print(\"Total sample number:\",total_sample_num)\n",
    "    print(f\"There are {problem_sample_num} samples that failed to be evaluated.\")\n",
    "    print(\"-\"*20)\n",
    "\n",
    "def evaluate_tabmcq_questions(benchmark_name,pred_item_list):\n",
    "    \"\"\"\n",
    "    Evaluation for TabMCQ benchmark.\n",
    "    Metric: accuracy. \n",
    "    \"\"\"\n",
    "    correct_item_list = []\n",
    "    wrong_item_list = []\n",
    "    failed_item_list = []\n",
    "    \n",
    "    for item in pred_item_list:\n",
    "        try:\n",
    "            item_id = item['question_id']\n",
    "            ori_item = item_id_to_test_item[item_id]\n",
    "            model_output = item['text']\n",
    "            model_output = model_output.replace('\\n',' ')\n",
    "            model_output = model_output.replace(']','')\n",
    "            model_output = model_output.replace('[','')\n",
    "            ret = re.match('.*({[\\\"\\']answer[\\\"\\']\\:\\s?.*?}).*',model_output)\n",
    "            # parse predicted answer\n",
    "            if ret is not None:\n",
    "                answer_str = ret.group(1)\n",
    "                answer_item = eval(answer_str)\n",
    "                predicted_answer = answer_item['answer']\n",
    "                if type(predicted_answer) == list:\n",
    "                    predicted_answer = predicted_answer[0]\n",
    "                gold_answer_str = ori_item['answer_list'][0] # e.g., '(D) Blood'\n",
    "                # Sometimes the predicted answer does not contain option letter like '(D)'\n",
    "                # To deal with such cases, we also consider removing the option letter in ground truth for comparison\n",
    "                if predicted_answer == gold_answer_str or predicted_answer == ' '.join(gold_answer_str.split(' ')[1:]):\n",
    "                    correct_item_list.append(item)\n",
    "                else:\n",
    "                    wrong_item_list.append(item)\n",
    "            else:\n",
    "                failed_item_list.append(item)\n",
    "        except Exception:\n",
    "            failed_item_list.append(item)\n",
    "            \n",
    "    print(f\"Benchmark: {benchmark_name}\")\n",
    "    total_sample_num = len(pred_item_list)\n",
    "    correct_num = len(correct_item_list)\n",
    "    print(\"Accuracy: \",correct_num/total_sample_num)\n",
    "    problem_sample_num = len(failed_item_list)\n",
    "    print(\"Total sample number:\",total_sample_num)\n",
    "    print(f\"There are {problem_sample_num} samples that failed to be evaluated.\")\n",
    "    print(\"-\"*20)\n",
    "\n",
    "def evaluate_text_generation_questions(benchmark_name,pred_item_list):\n",
    "    \"\"\"\n",
    "    Evaluation for table-to-text benchmark.\n",
    "    Metric: bleu.\n",
    "    More metrics like ROUGE or LLM-as-a-judge rating are needed for a more robust evaluation.\n",
    "    \"\"\"\n",
    "    bleu = BLEU()\n",
    "    output_text_list = [] # output text \n",
    "    reference_text_list = [] # reference text list\n",
    "    for item in pred_item_list:\n",
    "        pred_text = item['text']\n",
    "        item_id = item['question_id']\n",
    "        ori_item = item_id_to_test_item[item_id]\n",
    "        gold_text = ori_item['output']\n",
    "        assert gold_text not in ['','None']\n",
    "        output_text_list.append(pred_text)\n",
    "        reference_text_list.append(gold_text)\n",
    "    assert len(output_text_list) == len(reference_text_list)\n",
    "    bleu_score = bleu.corpus_score(output_text_list, [reference_text_list])\n",
    "    print(\"Benchmark: \",benchmark_name)\n",
    "    print(\"BLEU score:\",bleu_score)\n",
    "    print(\"-\"*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Table Structure Understanding (TSU) Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TEDS object, 'n_jobs' is the number of parallel threads \n",
    "teds = TEDS(n_jobs=36)\n",
    "def evaluate_tr_questions(pred_item_list):\n",
    "    \"\"\"\n",
    "    Evaluation for table recognition (TR) benchmark.\n",
    "    Metric: TEDS score (Tree-Edit-Distance-based Similarity)\n",
    "    We directly use the TEDS object from the PubTabNet code to compute TEDS score based on html table string.\n",
    "    For test samples with latex and markdown table format, we convert them into html table string to compute TEDS score.\n",
    "    Source of TEDS computation code: https://github.com/ibm-aur-nlp/PubTabNet/tree/master/src\n",
    "    \"\"\"\n",
    "    html_item_list = []\n",
    "    markdown_item_list = []\n",
    "    latex_item_list = []\n",
    "    item_id_to_pred = {}\n",
    "    item_id_to_gold = {}\n",
    "    failed_item_list = [] # store items that we failed to extract or build html table string\n",
    "    \n",
    "    for item in pred_item_list:\n",
    "        try:\n",
    "            item_id = item['question_id']\n",
    "            ori_item = item_id_to_test_item[item_id]\n",
    "            query_type = ori_item['original_query_type'] # e.g., 'table recognition with HTML', 'table recognition with Latex'\n",
    "            item['query_type'] = query_type\n",
    "            pred_table_repr = item['text']\n",
    "            gold_table_repr = ori_item['output']\n",
    "            # seperate different table recognition questions based on 'query_type' field\n",
    "            if 'HTML' in query_type:\n",
    "                # extract html str from the output using regular expression\n",
    "                pred_table_repr = pred_table_repr.replace('\\n','')\n",
    "                if \"<body\" in pred_table_repr:\n",
    "                    pred_table_repr = re.findall('<body.*',pred_table_repr)[0]\n",
    "                elif \"<table\" in pred_table_repr:\n",
    "                    pred_table_repr = re.findall('<table.*',pred_table_repr)[0]\n",
    "                else:\n",
    "                    failed_item_list.append(item)\n",
    "                    continue\n",
    "                pred_html_table = wrap_html_table(pred_table_repr)\n",
    "                gold_html_table = wrap_html_table(gold_table_repr)\n",
    "            elif 'Latex' in query_type:\n",
    "                # convert latex table string to html table for TEDS computation\n",
    "                # note: for now, this conversion only support flat table with the first row as column headers.\n",
    "                pred_html_table = convert_latex_table_to_html(pred_table_repr)\n",
    "                gold_html_table = convert_latex_table_to_html(gold_table_repr)\n",
    "            else:\n",
    "                # convert markdown table string to html table for TEDS computation\n",
    "                pred_html_table = convert_markdown_table_to_html(pred_table_repr)\n",
    "                gold_html_table = convert_markdown_table_to_html(gold_table_repr)\n",
    "            item['pred_html_table'] = pred_html_table\n",
    "            item['gold_html_table'] = gold_html_table\n",
    "            item_id_to_pred[item_id] = pred_html_table\n",
    "            item_id_to_gold[item_id] = gold_html_table\n",
    "        except Exception as e:\n",
    "            item['exception'] = e\n",
    "            failed_item_list.append(item)\n",
    "    # teds.batch_evaluate() returns a dictionary of {item_id: TEDS score}\n",
    "    item_id_to_teds_score = teds.batch_evaluate(item_id_to_pred, item_id_to_gold)\n",
    "    item_list_without_teds_score = [] \n",
    "    for item in pred_item_list:\n",
    "        item_id = item['question_id']\n",
    "        query_type = item['query_type']\n",
    "        if item_id in item_id_to_teds_score:\n",
    "            TEDS = item_id_to_teds_score[item_id]\n",
    "            if type(TEDS) != float:\n",
    "                item_list_without_teds_score.append(item)\n",
    "                continue\n",
    "            item['TEDS'] = TEDS\n",
    "            if 'HTML' in query_type:\n",
    "                html_item_list.append(item)\n",
    "            elif 'Latex' in query_type:\n",
    "                latex_item_list.append(item)\n",
    "            else:\n",
    "                markdown_item_list.append(item)\n",
    "        else:\n",
    "            item_list_without_teds_score.append(item)  \n",
    "    print(\"Benchmark: TR\")   \n",
    "    print(\"HTML sample number:\",len(html_item_list))\n",
    "    print(\"Markdown sample number:\",len(markdown_item_list))\n",
    "    print(\"Latex sample number:\",len(latex_item_list))\n",
    "    print(\"\")\n",
    "    if len(html_item_list) != 0:\n",
    "        html_ave_teds_score = sum([item['TEDS'] for item in html_item_list])/len(html_item_list)\n",
    "        print(\"Average TEDS score for HTML samples:\",html_ave_teds_score)\n",
    "    if len(markdown_item_list) != 0:\n",
    "        markdown_ave_teds_score = sum([item['TEDS'] for item in markdown_item_list])/len(markdown_item_list)\n",
    "        print(\"Average TEDS score for Markdown samples:\",markdown_ave_teds_score)\n",
    "    if len(latex_item_list) != 0:\n",
    "        latex_ave_teds_score = sum([item['TEDS'] for item in latex_item_list])/len(latex_item_list)\n",
    "        print(\"Average TEDS score for Latex samples:\",latex_ave_teds_score)\n",
    "        print(\"\")\n",
    "        \n",
    "    total_sample_num = len(pred_item_list)\n",
    "    sample_num_without_teds_score = len(item_list_without_teds_score)\n",
    "    sample_num_without_html_table_string = len(failed_item_list)\n",
    "    print(\"Total sample number:\",total_sample_num)\n",
    "    print(f\"There are {sample_num_without_html_table_string} samples that we can not extract HTML table string.\")\n",
    "    print(f\"There are {sample_num_without_teds_score} samples that we can not compute TEDS score.\")\n",
    "    print(\"-\"*20)\n",
    "\n",
    "def evaluate_mcd_questions(benchmark_name,pred_item_list):\n",
    "    \"\"\"\n",
    "    Evaluation for merged cell detection (MCD) benchmark.\n",
    "    Metric: precision, recall and F1 score\n",
    "    \"\"\"\n",
    "    pred_cell_num = 0 # number of predicted merged cells \n",
    "    gold_cell_num = 0 # number of gold merged cells\n",
    "    correct_cell_num = 0 # number of predicted merged cells which are correct\n",
    "    failed_item_list = []\n",
    "    for item in pred_item_list:\n",
    "        try:\n",
    "            item_id = item['question_id']\n",
    "            ori_item = item_id_to_test_item[item_id]\n",
    "            model_output = item['text']\n",
    "            model_output = model_output.replace('\\n',' ')\n",
    "            gold_answer_list = ori_item['answer_list']\n",
    "            merged_cell_region_list = []\n",
    "            gold_cell_num += len(gold_answer_list)\n",
    "            if gold_answer_list == ['None']: # There are no merged cells in the table\n",
    "                # Different models may use different sentences to express 'there is no merged cell'.\n",
    "                # In such cases, you need to include more expressions for a more accurate evaluation.\n",
    "                if \"does not contain any merged cells\" in model_output.lower() or \"no merged cell\" in model_output.lower():\n",
    "                    correct_cell_num += 1\n",
    "                    pred_cell_num += 1\n",
    "                else:\n",
    "                    pred_cell_num += 1\n",
    "            else:  # There are merged cells in the table\n",
    "                # parse the ground truth coordinates of merged cells\n",
    "                for answer_str in gold_answer_list:\n",
    "                    gold_answer_item = eval(answer_str)\n",
    "                    top_row_id, left_col_id = gold_answer_item['top-left']\n",
    "                    bottom_row_id, right_col_id = gold_answer_item['bottom-right']\n",
    "                    gold_merged_region_repr = f\"{top_row_id}_{left_col_id}_{bottom_row_id}_{right_col_id}\"\n",
    "                    merged_cell_region_list.append(gold_merged_region_repr)\n",
    "                # parse the predicted coordinates of merged cells\n",
    "                pred_answer_str_list = re.findall('{[\\\"\\']top-left[\\\"\\']\\:.*?,\\s?[\\\"\\']bottom-right[\\\"\\']\\:.*?}',model_output)\n",
    "                for answer_str in pred_answer_str_list:\n",
    "                    pred_answer_item = eval(answer_str)\n",
    "                    top_row_id, left_col_id = pred_answer_item['top-left']\n",
    "                    bottom_row_id, right_col_id = pred_answer_item['bottom-right']\n",
    "                    pred_merged_region_repr = f\"{top_row_id}_{left_col_id}_{bottom_row_id}_{right_col_id}\"\n",
    "                    if pred_merged_region_repr in merged_cell_region_list:\n",
    "                        correct_cell_num += 1\n",
    "                pred_cell_num += len(pred_answer_str_list)\n",
    "        except Exception as e:\n",
    "            failed_item_list.append(item)\n",
    "            item['exception'] = e\n",
    "             \n",
    "    print(f\"Benchmark: {benchmark_name}\")\n",
    "    P = correct_cell_num / pred_cell_num\n",
    "    R = correct_cell_num / gold_cell_num\n",
    "    print(\"Precision:\",P)\n",
    "    print(\"Recall:\",R)\n",
    "    if P+R == 0:\n",
    "        F1 = 0\n",
    "    else:\n",
    "        F1 = 2*P*R/(P+R)\n",
    "    print(\"F1 score:\",F1)\n",
    "    total_sample_num = len(pred_item_list)\n",
    "    problem_sample_num = len(failed_item_list)\n",
    "    print(\"Total sample number:\",total_sample_num)\n",
    "    print(f\"There are {problem_sample_num} samples that failed to be evaluated.\")\n",
    "    print(\"-\"*20)\n",
    "\n",
    "def evaluate_tcl_questions(benchmark_name,pred_item_list):\n",
    "    \"\"\"\n",
    "    Evaluation for table cell locating (TCL) benchmark.\n",
    "    Metric: cell-level accuracy\n",
    "    \"\"\"\n",
    "    total_cell_num = 0\n",
    "    correct_cell_num = 0\n",
    "    failed_item_list = []\n",
    "    for item in pred_item_list:\n",
    "        try:\n",
    "            item_id = item['question_id']\n",
    "            ori_item = item_id_to_test_item[item_id]\n",
    "            model_output = item['text']\n",
    "            model_output = model_output.replace('\\n',' ')\n",
    "            model_output = model_output.replace('\\\\','')\n",
    "            gold_output = ori_item['output']\n",
    "            # parse the ground truth cell locations (row_id, column_id)\n",
    "            # example gold_dict_str_list = [('Raúl Hidalgo', '(13, 1)'),\n",
    "            #                               ('Year', 'DOES NOT EXIST')],\n",
    "            gold_dict_str_list = re.findall('{[\\\"\\']value[\\\"\\']\\:\\s?[\\\"\\'](.*?)[\\\"\\'],\\s?[\\\"\\']location[\\\"\\']\\:\\s?[\\\"\\']?(.*?)[\\\"\\']?}',gold_output)\n",
    "            cell_str_to_location = {}\n",
    "            for cell_str,location_str in gold_dict_str_list:\n",
    "                cell_str_to_location[cell_str] = location_str\n",
    "            total_cell_num += len(cell_str_to_location)\n",
    "            item['cell_str_to_gold_location'] = cell_str_to_location\n",
    "            # parse the predicted cell locations\n",
    "            pred_dict_str_list = re.findall('{[\\\"\\']value[\\\"\\']\\:\\s?[\\\"\\'](.*?)[\\\"\\'],\\s?[\\\"\\']location[\\\"\\']\\:\\s?[\\\"\\']?(.*?)[\\\"\\']?}',model_output)\n",
    "            cell_str_to_pred_location = {}\n",
    "            for cell_str,location_str in pred_dict_str_list:\n",
    "                if cell_str in cell_str_to_location:\n",
    "                    gold_cell_location = cell_str_to_location[cell_str]\n",
    "                    pred_cell_location = location_str\n",
    "                    cell_str_to_pred_location[cell_str] = location_str\n",
    "                    if str(gold_cell_location).lower() == str(pred_cell_location).lower():\n",
    "                        correct_cell_num += 1 \n",
    "            item['cell_str_to_pred_location'] = cell_str_to_pred_location\n",
    "                \n",
    "        except Exception as e:\n",
    "            failed_item_list.append(item)\n",
    "            item['exception'] = e\n",
    "            \n",
    "    print(f\"Benchmark: {benchmark_name}\")\n",
    "    print(\"Cell-level accuracy:\",correct_cell_num/total_cell_num)\n",
    "    total_sample_num = len(pred_item_list)\n",
    "    problem_sample_num = len(failed_item_list)\n",
    "    print(\"Total sample number:\",total_sample_num)\n",
    "    print(f\"There are {problem_sample_num} samples that failed to be evaluated.\")\n",
    "    print(\"-\"*20)\n",
    "    \n",
    "def evaluate_rce_questions(benchmark_name,pred_item_list):\n",
    "    \"\"\"\n",
    "    Evaluation for row and column extraction (RCE) benchmark.\n",
    "    Metric: row and column level F1 score\n",
    "    \"\"\"\n",
    "    row_correct_cell_num = 0 \n",
    "    row_pred_cell_num = 0 \n",
    "    row_ori_cell_num = 0 \n",
    "    column_correct_cell_num = 0 \n",
    "    column_pred_cell_num = 0 \n",
    "    column_ori_cell_num = 0\n",
    "    \n",
    "    failed_item_list = []\n",
    "    \n",
    "    for item in pred_item_list:\n",
    "        try:\n",
    "            item_id = item['question_id']\n",
    "            ori_item = item_id_to_test_item[item_id]\n",
    "            image_id = ori_item['image_id']\n",
    "            table_rows = table_id_to_test_table[image_id]['table_rows']\n",
    "            model_output = item['text']\n",
    "            \n",
    "            row_id_to_correct_cell_list = {}\n",
    "            col_id_to_correct_cell_list = {}\n",
    "            # parse the predicted cells of specific row or column\n",
    "            match_group_tuple_list = re.findall('{[\\\"\\']row_id[\\\"\\']\\:(.*?),\\s?[\\\"\\']cell_list[\\\"\\']\\:(.*?)}|{[\\\"\\']column_id[\\\"\\']\\:(.*?),\\s?[\\\"\\']cell_list[\\\"\\']\\:(.*?)}',model_output)\n",
    "            for matched_tuple in match_group_tuple_list:\n",
    "                if matched_tuple[0] != '': # extract cells from a specific row\n",
    "                    row_id = int(eval(matched_tuple[0]))\n",
    "                    pred_cell_list = eval(matched_tuple[1])\n",
    "                    target_cell_list = table_rows[row_id-1] # the ground truth cells in the original row\n",
    "                    row_pred_cell_num += len(pred_cell_list)\n",
    "                    row_ori_cell_num += len(target_cell_list)\n",
    "                    correct_cell_list = [c for c in pred_cell_list if c in target_cell_list] # predicted cells that are also in the ground truth\n",
    "                    row_correct_cell_num += len(correct_cell_list)\n",
    "                    row_id_to_correct_cell_list[row_id] = correct_cell_list\n",
    "                else: # extract cells from a specific column\n",
    "                    column_id = int(eval(matched_tuple[2]))\n",
    "                    pred_cell_list = eval(matched_tuple[3])\n",
    "                    target_cell_list = [] # the ground truth cells in the original column  \n",
    "                    for row in table_rows:\n",
    "                        if len(row) == 1:\n",
    "                            target_cell_list.append(row[0])\n",
    "                        else:\n",
    "                            target_cell_list.append(row[column_id-1])\n",
    "                    column_pred_cell_num += len(pred_cell_list)\n",
    "                    column_ori_cell_num += len(target_cell_list)\n",
    "                    correct_cell_list = [c for c in pred_cell_list if c in target_cell_list] # predicted cells that are also in the ground truth\n",
    "                    column_correct_cell_num += len(correct_cell_list)\n",
    "                    col_id_to_correct_cell_list[column_id] = correct_cell_list\n",
    "            item['row_id_to_correct_cell_list'] = row_id_to_correct_cell_list\n",
    "            item['col_id_to_correct_cell_list'] = col_id_to_correct_cell_list\n",
    "            \n",
    "        except Exception as e:\n",
    "            failed_item_list.append(item)\n",
    "            item['exception'] = e\n",
    "            \n",
    "    print(f\"Benchmark: {benchmark_name}\")\n",
    "    row_P = row_correct_cell_num/row_pred_cell_num # row-level precision\n",
    "    row_R = row_correct_cell_num/row_ori_cell_num # row-level recall\n",
    "    row_F1 = 2*row_P*row_R/(row_P+row_R) # row-level F1\n",
    "    col_P = column_correct_cell_num/column_pred_cell_num # column-level precision\n",
    "    col_R = column_correct_cell_num/column_ori_cell_num # column-level recall\n",
    "    col_F1 = 2*col_P*col_R/(col_P+col_R) # column-level F1\n",
    "    \n",
    "    print(\"Row-level Precision:\",row_P)\n",
    "    print(\"Row-level Recall:\",row_R)\n",
    "    print(\"Row-level F1:\",row_F1)\n",
    "    print(\"\")\n",
    "    print(\"Column-level Precision:\",col_P)\n",
    "    print(\"Column-level Recall:\",col_R)\n",
    "    print(\"Column-level F1:\",col_F1)\n",
    "    \n",
    "    total_sample_num = len(pred_item_list)\n",
    "    problem_sample_num = len(failed_item_list)\n",
    "    print(\"Total sample number:\",total_sample_num)\n",
    "    print(f\"There are {problem_sample_num} samples that failed to be evaluated.\")\n",
    "    print(\"-\"*20)\n",
    "\n",
    "def evaluate_tce_questions(benchmark_name,pred_item_list):\n",
    "    \"\"\"\n",
    "    Evaluation for table cell extraction (TCE) benchmark.\n",
    "    Metric: cell-level accuracy\n",
    "    \"\"\"\n",
    "    total_cell_num = 0\n",
    "    correct_cell_num = 0\n",
    "    failed_item_list = []\n",
    "    for item in pred_item_list:\n",
    "        try:\n",
    "            item_id = item['question_id']\n",
    "            ori_item = item_id_to_test_item[item_id]\n",
    "            model_output = item['text']\n",
    "            model_output = model_output.replace('\\n',' ')\n",
    "            gold_output = ori_item['output']\n",
    "            # parse ground truth cell value\n",
    "            gold_dict_str_list = re.findall('{[\\\"\\']row_id[\\\"\\']\\:.*?[\\\"\\']column_id[\\\"\\']\\:.*?[\\\"\\']cell_value[\\\"\\']\\:.*?}',gold_output)\n",
    "            cell_location_to_cell_str = {}\n",
    "            for dict_str in gold_dict_str_list:\n",
    "                cell_item = eval(dict_str)\n",
    "                row_id = cell_item['row_id']\n",
    "                col_id = cell_item['column_id']\n",
    "                gold_cell_value = cell_item['cell_value']\n",
    "                cell_location_to_cell_str[f\"{row_id}_{col_id}\"] = gold_cell_value\n",
    "            total_cell_num += len(cell_location_to_cell_str)\n",
    "            # parse predicted cell value\n",
    "            pred_dict_str_list = re.findall('{[\\\"\\']row_id[\\\"\\']\\:.*?[\\\"\\']column_id[\\\"\\']\\:.*?[\\\"\\']cell_value[\\\"\\']\\:.*?}',model_output)\n",
    "            cell_location_to_pred_str = {}\n",
    "            for dict_str in pred_dict_str_list:\n",
    "                # some output may contain extra '[' or ']'\n",
    "                dict_str = dict_str.replace(']','')\n",
    "                dict_str = dict_str.replace('[','')\n",
    "                cell_item = eval(dict_str)\n",
    "                row_id = cell_item['row_id']\n",
    "                col_id = cell_item['column_id']\n",
    "                cell_location = f\"{row_id}_{col_id}\"\n",
    "                if (cell_location in cell_location_to_cell_str) and (cell_location not in cell_location_to_pred_str) :\n",
    "                    gold_cell_value = cell_location_to_cell_str[cell_location]\n",
    "                    pred_cell_value = cell_item['cell_value']\n",
    "                    cell_location_to_pred_str[cell_location] = pred_cell_value\n",
    "                    if str(pred_cell_value).lower() == str(gold_cell_value).lower():\n",
    "                        correct_cell_num += 1\n",
    "    \n",
    "        except Exception as e:\n",
    "            failed_item_list.append(item)\n",
    "            item['exception'] = e\n",
    "            \n",
    "    print(f\"Benchmark: {benchmark_name}\")\n",
    "    print(\"cell level accuracy: \",correct_cell_num/total_cell_num)\n",
    "    total_sample_num = len(pred_item_list)\n",
    "    problem_sample_num = len(failed_item_list)\n",
    "    print(\"Total sample number:\",total_sample_num)\n",
    "    print(f\"There are {problem_sample_num} samples that failed to be evaluated.\")\n",
    "    print(\"-\"*20)\n",
    "\n",
    "def evaluate_tsd_questions(benchmark_name,pred_item_list):\n",
    "    \"\"\"\n",
    "    Evaluation for table size detection (TSD) benchmark.\n",
    "    Metric: row number and column number accuracy\n",
    "    \"\"\"\n",
    "    row_correct_num = 0\n",
    "    col_correct_num = 0\n",
    "    failed_item_list = []\n",
    "    \n",
    "    for item in pred_item_list:\n",
    "        try:\n",
    "            item_id = item['question_id']\n",
    "            ori_item = item_id_to_test_item[item_id]\n",
    "            model_output = item['text'].lower()\n",
    "            model_output = model_output.replace('\\n',' ')\n",
    "            model_output = model_output.replace('\\\\','')\n",
    "            # parse predicted row number and column number\n",
    "            if 'row_number' in model_output:\n",
    "                ret = re.match('.*({.*[\\\"\\']row_number[\\\"\\']:.*[\\\"\\']column_number[\\\"\\']:.*}).*',model_output)\n",
    "                answer_str = ret.group(1)\n",
    "                answer_item = eval(answer_str)\n",
    "                pred_row_number = answer_item['row_number']\n",
    "                pred_col_number = answer_item['column_number']\n",
    "            else:\n",
    "                ret = re.match('.*(\\d+) rows and (\\d+) columns.*',model_output)\n",
    "                pred_row_number = ret.group(1)\n",
    "                pred_col_number = ret.group(2)\n",
    "            # extract ground truth row number and column number\n",
    "            gold_answer_tuple = ori_item['answer_list'][0]\n",
    "            gold_row_number = str(gold_answer_tuple[0])\n",
    "            gold_col_number = str(gold_answer_tuple[1])\n",
    "            if pred_row_number == gold_row_number:\n",
    "                row_correct_num += 1\n",
    "            if pred_col_number == gold_col_number:\n",
    "                col_correct_num += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            item['exception'] = e\n",
    "            failed_item_list.append(item)\n",
    "            \n",
    "    print(f\"Benchmark: {benchmark_name}\")\n",
    "    total_sample_num = len(pred_item_list)\n",
    "    print(\"row number accuracy:\",row_correct_num/total_sample_num)\n",
    "    print(\"column number accuracy:\",col_correct_num/total_sample_num)\n",
    "    problem_sample_num = len(failed_item_list)\n",
    "    print(\"Total sample number:\",total_sample_num)\n",
    "    print(f\"There are {problem_sample_num} samples that failed to be evaluated.\")\n",
    "    print(\"-\"*20)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Automatic Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark: TSD\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m predicted_item_list \u001b[38;5;241m=\u001b[39m benchmark_name_to_predicted_item_list[benchmark_name]\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m benchmark_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTSD\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOOD_TSD\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m----> 7\u001b[0m     \u001b[43mevaluate_tsd_questions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbenchmark_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpredicted_item_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m benchmark_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTCE\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOOD_TCE\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m      9\u001b[0m     evaluate_tce_questions(benchmark_name,predicted_item_list)\n",
      "Cell \u001b[0;32mIn[11], line 388\u001b[0m, in \u001b[0;36mevaluate_tsd_questions\u001b[0;34m(benchmark_name, pred_item_list)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBenchmark: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbenchmark_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    387\u001b[0m total_sample_num \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(pred_item_list)\n\u001b[0;32m--> 388\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrow number accuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[43mrow_correct_num\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43mtotal_sample_num\u001b[49m)\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn number accuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m,col_correct_num\u001b[38;5;241m/\u001b[39mtotal_sample_num)\n\u001b[1;32m    390\u001b[0m problem_sample_num \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(failed_item_list)\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "benchmark_name_list = ['TSD','TCL','RCE','MCD','TCE','OOD_TSD','OOD_TCE','OOD_TCL','OOD_RCE','TABMWP','WTQ','HiTab','TAT-QA',\n",
    "                      'TabFact','InfoTabs','AIT-QA','PubHealthTab','TabMCQ',\n",
    "                      'FeTaQA','HiTab_t2t','Rotowire','WikiBIO']\n",
    "for benchmark_name in benchmark_name_list:\n",
    "    predicted_item_list = benchmark_name_to_predicted_item_list[benchmark_name]\n",
    "    if benchmark_name in ['TSD','OOD_TSD']:\n",
    "        evaluate_tsd_questions(benchmark_name,predicted_item_list)\n",
    "    elif benchmark_name in ['TCE','OOD_TCE']:\n",
    "        evaluate_tce_questions(benchmark_name,predicted_item_list)\n",
    "    elif benchmark_name in ['TCL','OOD_TCL']:\n",
    "        evaluate_tcl_questions(benchmark_name,predicted_item_list)\n",
    "    elif benchmark_name in ['RCE','OOD_RCE']:\n",
    "        evaluate_rce_questions(benchmark_name,predicted_item_list)\n",
    "    elif benchmark_name == 'MCD':\n",
    "        evaluate_mcd_questions(benchmark_name,predicted_item_list)\n",
    "    elif benchmark_name == 'TabMCQ':\n",
    "        evaluate_tabmcq_questions(benchmark_name,predicted_item_list)\n",
    "    elif benchmark_name in ['FeTaQA','HiTab_t2t','Rotowire','WikiBIO']:\n",
    "        evaluate_text_generation_questions(benchmark_name,predicted_item_list)\n",
    "    else:\n",
    "        evaluate_tqa_questions(benchmark_name,predicted_item_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark:  WTQ\n",
      "Accuracy:  0.0\n",
      "Total sample number: 4344\n",
      "There are 4344 samples that failed to be evaluated.\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "benchmark_name = 'WTQ'\n",
    "\n",
    "evaluate_tqa_questions(benchmark_name,benchmark_name_to_predicted_item_list[benchmark_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark: TR\n",
      "HTML sample number: 0\n",
      "Markdown sample number: 0\n",
      "Latex sample number: 0\n",
      "\n",
      "Total sample number: 0\n",
      "There are 0 samples that we can not extract HTML table string.\n",
      "There are 0 samples that we can not compute TEDS score.\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# The evaluation of table recognition task takes about 2.5 minutes.\n",
    "benchmark_name = 'TR'\n",
    "predicted_item_list = benchmark_name_to_predicted_item_list[benchmark_name]\n",
    "evaluate_tr_questions(predicted_item_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ToTT test sample number: 7700\n"
     ]
    }
   ],
   "source": [
    "# For ToTTo benchmark, you need to upload the predicted results to the offical LeaderBoard for evaluation.\n",
    "# See the ToTTo github for details: https://github.com/google-research-datasets/ToTTo#leaderboard-submission\n",
    "# Process the model output results of ToTTo test set for evaluation.\n",
    "benchmark_name = 'ToTTo'\n",
    "pred_item_list = benchmark_name_to_predicted_item_list[benchmark_name]\n",
    "# Sort the output samples\n",
    "totto_sample_index_to_item = {}\n",
    "for item in pred_item_list:\n",
    "    item_id = item['question_id']\n",
    "    sample_index = int(item_id.strip('ToTTo_test_item_'))\n",
    "    totto_sample_index_to_item[sample_index] = item\n",
    "# Write the output result to a txt file. Each line represents a output text for a sample.\n",
    "# The sample order must be the same as the original ToTTo test set.\n",
    "sorted_totto_item_list = []\n",
    "model_name = 'Table-LLaVA_7B'\n",
    "# Use this txt file for leaderboard submission\n",
    "with open(f'./ToTTo_test_results_{model_name}.txt','w',encoding='utf-8') as f:\n",
    "    for sample_index in range(len(pred_item_list)):\n",
    "        item = totto_sample_index_to_item[sample_index]\n",
    "        output_text = item['text'].replace('\\n',' ').replace('\\t',' ')\n",
    "        f.write(output_text+'\\n')\n",
    "        sorted_totto_item_list.append(item)\n",
    "print(\"ToTT test sample number:\",len(sorted_totto_item_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cotable",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
